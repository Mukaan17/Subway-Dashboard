services:
  # Zookeeper for Kafka - optimized for GCP
  zookeeper:
    image: confluentinc/cp-zookeeper:7.3.0
    platform: linux/amd64
    container_name: zookeeper
    ports:
      - "2181:2181"
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
      KAFKA_HEAP_OPTS: "-Xmx128M -Xms64M"
    restart: unless-stopped
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "3"
    deploy:
      resources:
        limits:
          cpus: '0.3'
          memory: 200M

  # Kafka broker - optimized for GCP e2-standard-2
  kafka:
    image: confluentinc/cp-kafka:7.3.0
    platform: linux/amd64
    container_name: kafka
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_LOG_RETENTION_HOURS: 24
      KAFKA_LOG_RETENTION_BYTES: 1073741824  # 1GB
      KAFKA_NUM_PARTITIONS: 1
      KAFKA_HEAP_OPTS: "-Xmx512M -Xms256M"
      KAFKA_JVM_PERFORMANCE_OPTS: "-XX:+UseG1GC -XX:MaxGCPauseMillis=20 -XX:InitiatingHeapOccupancyPercent=35"
      KAFKA_GC_LOG_OPTS: "-Xlog:gc*:file=/var/log/kafka/gc.log:time,tags:filecount=5,filesize=102400"
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "3"
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 600M
    healthcheck:
      test: ["CMD", "kafka-topics", "--bootstrap-server", "localhost:9092", "--list"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  # MongoDB - optimized for GCP
  mongodb:
    image: mongo:4.4
    container_name: mongodb
    command: ["mongod", "--wiredTigerCacheSizeGB", "0.5"]
    ports:
      - "27017:27017"
    volumes:
      - mongo_data:/data/db
    restart: unless-stopped
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "3"
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 600M
    environment:
      - MONGO_INITDB_DATABASE=subway_dash
    healthcheck:
      test: ["CMD", "mongo", "--eval", "db.adminCommand('ping')"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  # MTA Data Producer - optimized for GCP
  producer:
    build:
      context: .
      dockerfile: src/producer/Dockerfile
    container_name: mta-producer
    depends_on:
      kafka:
        condition: service_healthy
    env_file:
      - .env
    volumes:
      - ./src/producer:/app
    restart: unless-stopped
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "3"
    deploy:
      resources:
        limits:
          cpus: '0.3'
          memory: 200M

  # Spark Master - optimized for GCP
  spark-master:
    image: bitnami/spark:3.3.1
    container_name: spark-master
    ports:
      - "8090:8080"
      - "7077:7077"
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - SPARK_DAEMON_MEMORY=512m
    restart: unless-stopped
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "3"
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 600M
    healthcheck:
      test: ["CMD-SHELL", "wget -q -O - http://localhost:8080/json/ | grep -q '\"status\"' || exit 1"]


  # Spark Worker - optimized for GCP e2-standard-2
  spark-worker:
    image: bitnami/spark:3.3.1
    container_name: spark-worker
    depends_on:
      spark-master:
        condition: service_healthy
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=512M
      - SPARK_WORKER_CORES=1
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - SPARK_DAEMON_MEMORY=256m
    restart: unless-stopped
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "3"
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 600M
    tmpfs:
      - /tmp/spark-temp:rw,noexec,nosuid,size=250m
      - /tmp/spark-work:rw,noexec,nosuid,size=250m

  # Spark Processor - optimized for GCP
  processor:
    build:
      context: .
      dockerfile: src/processor/Dockerfile
    container_name: mta-processor
    depends_on:
      kafka:
        condition: service_healthy
      spark-master:
        condition: service_healthy
      mongodb:
        condition: service_healthy
    volumes:
      - ./src/processor:/app
    restart: unless-stopped
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "3"
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 400M
    environment:
      - SPARK_LOCAL_DIRS=/tmp/spark-temp
      - SPARK_WORKER_DIR=/tmp/spark-work
      - SPARK_EXECUTOR_MEMORY=256m
      - SPARK_DRIVER_MEMORY=256m
    tmpfs:
      - /tmp/spark-temp:rw,noexec,nosuid,size=250m
      - /tmp/spark-work:rw,noexec,nosuid,size=250m

  # API Backend - optimized for GCP
  api:
    build:
      context: .
      dockerfile: src/api/Dockerfile
    container_name: mta-api
    depends_on:
      mongodb:
        condition: service_healthy
    ports:
      - "8000:8000"
    volumes:
      - ./src/api:/app
    restart: unless-stopped
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "3"
    deploy:
      resources:
        limits:
          cpus: '0.3'
          memory: 300M
    environment:
      - MAX_WORKERS=2
      - PYTHONUNBUFFERED=1
      - PYTHONDONTWRITEBYTECODE=1
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/"]
      interval: 30s
      timeout: 10s
      retries: 3

  # React Dashboard - production mode
  dashboard:
    build:
      context: ./dashboard
      dockerfile: Dockerfile
      args:
        - NODE_ENV=production
    container_name: mta-dashboard
    depends_on:
      api:
        condition: service_healthy
    ports:
      - "3000:80"
    environment:
      - REACT_APP_API_URL=https://mta-dashboard.real-time.work/api/api
      - NODE_ENV=production
    restart: unless-stopped
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "3"
    deploy:
      resources:
        limits:
          cpus: '0.2'
          memory: 200M
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost"]
      interval: 30s
      timeout: 10s
      retries: 3
      
volumes:
  mongo_data:
    driver: local
